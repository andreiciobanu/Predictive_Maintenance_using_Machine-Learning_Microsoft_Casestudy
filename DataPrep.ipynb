{"cells":[{"cell_type":"markdown","source":["# Data Sources\n","\n","Common data sources for predictive maintenance problems are :\n","\n","* **Failure history:** The failure history of a machine or component within the machine.\n","* **Maintenance history:** The repair history of a machine, e.g. error codes, previous maintenance activities or component replacements.\n","* **Machine conditions and usage:** The operating conditions of a machine e.g. data collected from sensors.\n","* **Machine features:** The features of a machine, e.g. engine size, make and model, location.\n","* **Operator features:** The features of the operator, e.g. gender, past experience\n","The data for this example comes from 4 different sources which are real-time telemetry data collected from machines, error messages, historical maintenance records that include failures and machine information such as type and age."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n","\n","data_dir='data/'\n","telemetry = pd.read_csv(data_dir + 'PdM_telemetry.csv')\n","errors = pd.read_csv(data_dir + 'PdM_errors.csv')\n","maint = pd.read_csv(data_dir + 'PdM_maint.csv')\n","failures = pd.read_csv(data_dir + 'PdM_failures.csv')\n","machines = pd.read_csv(data_dir + 'PdM_machines.csv')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"gather":{"logged":1598808334548}}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","source":["# format datetime field which comes in as string\n","# the standard deviation is a measure of the amount of variation or dispersion of a set of values.A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.\n","\n","telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n","\n","print(\"Total number of telemetry records: %d\" % len(telemetry.index))\n","print(telemetry)\n","telemetry.describe()\n"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808340189},"tags":[]}},{"cell_type":"markdown","source":["#### **Telemetry**\n","The first data source is the telemetry time-series data which consists of **voltage, rotation, pressure, and vibration** measurements collected from 100 machines in **real time averaged over every hour collected during the year 2015**. Below, we display the first 10 records in the dataset. A summary of the whole dataset is also provided."],"metadata":{}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#Access a group of rows and columns by label(s) or a boolean array.\n","plot_df = telemetry.loc[(telemetry['machineID'] == 1) & \n","                        (telemetry['datetime'] > pd.to_datetime('2015-01-01')) & \n","                        (telemetry['datetime'] <pd.to_datetime('2015-03-01')),\n","                        ['datetime','volt']]\n","\n","print(plot_df.head())\n","plot_df.describe()\n","sns.set_style(\"darkgrid\")\n","plt.figure(figsize=(20, 8))\n","plt.plot(plot_df['datetime'], plot_df['volt'])\n","plt.ylabel('voltage')\n","\n","# make x-axis ticks legible\n","adf = plt.gca().get_xaxis().get_major_formatter()\n","adf.scaled[1.0] = '%m-%d-%Y'\n","plt.xlabel('Date')"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808348579},"tags":[]}},{"cell_type":"markdown","source":["#### **Errors**\n","The second major data source is the error logs. These are **non-breaking errors thrown while the machine is still operational and do not constitute as failures.** The **error date and times** are rounded to the closest hour since the telemetry data is collected at an hourly rate."],"metadata":{}},{"cell_type":"code","source":["# format of datetime field which comes in as string\n","errors['datetime'] = pd.to_datetime(errors['datetime'],format = '%Y-%m-%d %H:%M:%S')\n","errors['errorID'] = errors['errorID'].astype('category')\n","print(\"Total Number of error records: %d\" %len(errors.index))\n","print(errors.head())"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808386513},"tags":[]}},{"cell_type":"code","source":["sns.set_style(\"darkgrid\")\n","plt.figure(figsize=(20, 8))\n","errors['errorID'].value_counts().plot(kind='bar')\n","plt.ylabel('Count')\n","errors['errorID'].value_counts()"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808389653}}},{"cell_type":"markdown","source":["#### **Maintenance**\n","These are the **scheduled and unscheduled** maintenance records which correspond to both **regular inspection of components as well as failures.** A **record is generated if a component is replaced during the scheduled inspection or replaced due to a breakdown.** The **records that are created due to breakdowns will be called failures** which is explained in the later sections. Maintenance data has both 2014 and 2015 records."],"metadata":{}},{"cell_type":"code","source":["maint['datetime'] = pd.to_datetime(maint['datetime'], format='%Y-%m-%d %H:%M:%S')\n","maint['comp'] = maint['comp'].astype('category')\n","print(\"Total Number of maintenance Records: %d\" %len(maint.index))\n","print(maint.head())\n","\n","print(maint.value_counts())"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808398421},"tags":[]}},{"cell_type":"code","source":["sns.set_style(\"darkgrid\")\n","plt.figure(figsize=(10, 4))\n","maint['comp'].value_counts().plot(kind='bar')\n","plt.ylabel('Count')\n","maint['comp'].value_counts()"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808401266}}},{"cell_type":"markdown","source":["#### **Machines**\n","This data set includes some information about the machines: model type and age (years in service)."],"metadata":{}},{"cell_type":"code","source":["machines['model'] = machines['model'].astype('category')\n","\n","print(\"Total number of machines: %d\" % len(machines.index))\n","print(machines.head())\n","print(machines['model'].value_counts())"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808404262},"tags":[]}},{"cell_type":"code","source":["sns.set_style(\"darkgrid\")\n","plt.figure(figsize=(15, 6))\n","_, bins, _ = plt.hist([machines.loc[machines['model'] == 'model1', 'age'],\n","                       machines.loc[machines['model'] == 'model2', 'age'],\n","                       machines.loc[machines['model'] == 'model3', 'age'],\n","                       machines.loc[machines['model'] == 'model4', 'age']],\n","                       20, stacked=True, label=['model1', 'model2', 'model3', 'model4'])\n","plt.xlabel('Age (yrs)')\n","plt.ylabel('Count')\n","plt.legend()"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808407735}}},{"cell_type":"markdown","source":["#### **Failures**\n","These are the records of component replacements **due to failures.** Each record has a **date and time, machine ID, and failed component type.**"],"metadata":{}},{"cell_type":"code","source":["# format datetime field which comes in as string\n","failures['datetime'] = pd.to_datetime(failures['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n","failures['failure'] = failures['failure'].astype('category')\n","\n","print(\"Total number of failures: %d\" % len(failures.index))\n","#print(failures.value_counts())\n","print(failures['failure'].value_counts())\n","print(failures['machineID'].value_counts())\n","print(failures)"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808410507},"tags":[]}},{"cell_type":"code","source":["sns.set_style(\"darkgrid\")\n","plt.figure(figsize=(15, 4))\n","failures['failure'].value_counts().plot(kind='bar')\n","plt.ylabel('Count')\n","failures['failure'].value_counts()"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598808413069}}},{"cell_type":"markdown","source":["## Feature Engineering\n","The first step in predictive maintenance applications is feature engineering which requires bringing the different data sources together to create features that best describe a machines's health condition at a given point in time. In the next sections, several feature engineering methods are used to create features based on the properties of each data source."],"metadata":{}},{"cell_type":"markdown","source":["### Lag Features from Telemetry\n","Telemetry data almost always comes with time-stamps which makes it suitable for calculating lagging features. A common method is to pick a window size for the lag features to be created and compute rolling aggregate measures such as mean, standard deviation, minimum, maximum, etc. to represent the short term history of the telemetry over the lag window. In the following, rolling mean and standard deviation of the telemetry data over the last 3 hour lag window is calculated for every 3 hours."],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["telemetry"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["#columns: the column which the values will be aggregated on\n","pivot_df = pd.pivot_table(telemetry,\n","                index = 'datetime',\n","                columns = 'machineID',\n","                values = 'volt')\n","                \n","pivot_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Downsample the series into 3 hours bins and sum the values of the timestamps falling into a bin.\n","pivot_df.resample('3H', closed='left', label='right').agg('mean')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivot_df.resample('3H', closed='left', label='right').agg('mean').unstack()"]},{"cell_type":"code","source":["# Calculate mean values for telemetry features\n","# maybe there is a need for .tail().dropna()\n","temp = []\n","fields = ['volt', 'rotate', 'pressure', 'vibration']\n","for col in fields:\n","    temp.append(pd.pivot_table(telemetry,\n","                               index='datetime',\n","                               columns='machineID',\n","                               values=col).resample('3H', closed='left', label='right').agg('mean').unstack())\n","telemetry_mean_3h = pd.concat(temp, axis=1)\n","telemetry_mean_3h.columns = [i + 'mean_3h' for i in fields]\n","telemetry_mean_3h.reset_index(inplace=True)\n","telemetry_mean_3h"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809178398},"tags":[]}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["# repeat for standard deviation\n","temp = []\n","for col in fields:\n","    temp.append(pd.pivot_table(telemetry,\n","                               index='datetime',\n","                               columns='machineID',\n","                               values=col).resample('3H', closed='left', label='right').agg('std').unstack())\n","telemetry_sd_3h = pd.concat(temp, axis=1)\n","telemetry_sd_3h.columns = [i + 'sd_3h' for i in fields]\n","telemetry_sd_3h.reset_index(inplace=True)\n","\n","telemetry_sd_3h"]},{"cell_type":"markdown","source":["\n","For capturing a longer term effect, 24 hour lag features are also calculated as below."],"metadata":{}},{"cell_type":"code","source":["#.tail().dropna() maybe needed after agg\n","temp = []\n","fields = ['volt', 'rotate', 'pressure', 'vibration']\n","for col in fields:\n","    rmean=pd.pivot_table(telemetry,\n","                                    index='datetime',\n","                                    columns='machineID',\n","                                    values=col)\n","                                    \n","    temp.append(rmean.rolling(24).mean().resample('3H',\n","                                                    closed='left',\n","                                                    label='right').agg('first').unstack())\n","telemetry_mean_24h = pd.concat(temp, axis=1)\n","telemetry_mean_24h.columns = [i + 'mean_24h' for i in fields]\n","telemetry_mean_24h.reset_index(inplace=True)\n","telemetry_mean_24h = telemetry_mean_24h.loc[-telemetry_mean_24h['voltmean_24h'].isnull()]\n","# Notice that a 24h rolling average is not available at the earliest timepoints\n","telemetry_mean_24h"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809436514}}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# repeat for standard deviation\n","temp = []\n","fields = ['volt', 'rotate', 'pressure', 'vibration']\n","for col in fields:\n","    pivot = pd.pivot_table(telemetry,\n","                                    index='datetime',\n","                                    columns='machineID',\n","                                    values=col)\n","    # Append rows of other to the end of caller, returning a new object.\n","    temp.append(pivot.rolling(24).std().resample('3H',\n","                                                    closed='left',\n","                                                    label='right').agg('first').unstack())\n","# Concatenate pandas objects along a particular axis with optional set logic along the other axes.\n","telemetry_sd_24h = pd.concat(temp, axis=1)\n","telemetry_sd_24h.columns = [i + 'sd_24h' for i in fields]\n","# Access a group of rows and columns by label(s) or a boolean array.\n","telemetry_sd_24h = telemetry_sd_24h.loc[-telemetry_sd_24h['voltsd_24h'].isnull()]\n","telemetry_sd_24h.reset_index(inplace=True)\n","telemetry_sd_24h"]},{"cell_type":"markdown","source":["Next, the columns of the feature datasets created earlier are merged to create the final feature set from telemetry."],"metadata":{}},{"cell_type":"code","source":["# merge columns of feature sets created earlier\n","telemetry_feat = pd.concat([telemetry_mean_3h,\n","                            telemetry_sd_3h.iloc[:,  2:6],\n","                            telemetry_mean_24h.iloc[:, 2:6],\n","                            telemetry_sd_24h.iloc[:, 2:6]], axis=1).dropna()\n","telemetry_feat"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809530071}}},{"cell_type":"markdown","source":["### Lag Features from Errors\n","Like telemetry data, errors come with timestamps. An important difference is that the **error IDs are categorical values** and **should not be averaged over time intervals like the telemetry measurements.** Instead, we count the number of errors of each type in a **lagging window. We begin by reformatting the error data** to have one entry per machine per time at which at least one error occurred:"],"metadata":{}},{"cell_type":"code","source":["errors"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809539250}}},{"cell_type":"code","source":["# create a column for each error type\n","# Convert categorical variable into dummy/indicator variables.\n","error_count = pd.get_dummies(errors.set_index('datetime')).reset_index()\n","error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5']\n","# Group DataFrame using a mapper or by a Series of columns.\n","# A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.\n","# combine errors for a given machine in a given hour\n","error_count = error_count.groupby(['machineID','datetime']).sum().reset_index()\n","#Merge DataFrame or named Series objects with a database-style join.\n","#The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on.\n","error_count = telemetry[['datetime', 'machineID']].merge(error_count, on=['machineID', 'datetime'], how='left').fillna(0.0)\n","error_count"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809547151}}},{"cell_type":"markdown","source":["Finally, we can compute the total number of errors of each type over the last 24 hours, for timepoints taken every three hours:"],"metadata":{}},{"cell_type":"code","source":["temp = []\n","fields = ['error%d' % i for i in range(1,6)]\n","for col in fields:\n","    pivot = pd.pivot_table(error_count,\n","                                        index='datetime',\n","                                        columns='machineID',\n","                                        values=col)\n","    temp.append(pivot.rolling(24).sum().resample('3H',\n","                                                closed='left',\n","                                                label='right').agg('first').unstack())\n","error_count = pd.concat(temp, axis=1)\n","error_count.columns = [i + 'count' for i in fields]\n","error_count.reset_index(inplace=True)\n","#Remove missing values.\n","error_count = error_count.dropna()\n","error_count"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809590076}}},{"cell_type":"markdown","source":["### Days Since Last Replacement from Maintenance\n","A crucial data set in this example is the maintenance records which contain the information of component replacement records. Possible features from this data set can be, for example, the number of replacements of each component in the last 3 months to incorporate the frequency of replacements. However, more relevent information would be to calculate how long it has been since a component is last replaced as that would be expected to correlate better with component failures since the longer a component is used, the more degradation should be expected.\n","\n","As a side note, creating lagging features from maintenance data is not as straightforward as for telemetry and errors, so the features from this data are generated in a more custom way. This type of ad-hoc feature engineering is very common in predictive maintenance since domain knowledge plays a big role in understanding the predictors of a problem. In the following, the days since last component replacement are calculated for each component type as features from the maintenance data."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n","\n","# create a column for each error type\n","comp_rep = pd.get_dummies(maint.set_index('datetime')).reset_index()\n","comp_rep.columns = ['datetime', 'machineID', 'comp1', 'comp2', 'comp3', 'comp4']\n","\n","# combine repairs for a given machine in a given hour\n","comp_rep = comp_rep.groupby(['machineID', 'datetime']).sum().reset_index()\n","\n","# add timepoints where no components were replaced\n","comp_rep = telemetry[['datetime', 'machineID']].merge(comp_rep,\n","                                                      on=['datetime', 'machineID'],\n","                                                      how='outer').fillna(0).sort_values(by=['machineID', 'datetime'])\n","\n","components = ['comp1', 'comp2', 'comp3', 'comp4']\n","for comp in components:\n","    # convert indicator to most recent date of component change\n","    comp_rep.loc[comp_rep[comp] < 1, comp] = None\n","    comp_rep.loc[-comp_rep[comp].isnull(), comp] = comp_rep.loc[-comp_rep[comp].isnull(), 'datetime']\n","    \n","    # forward-fill the most-recent date of component change\n","    comp_rep[comp] = comp_rep[comp].fillna(method='ffill')\n","\n","# remove dates in 2014 (may have NaN or future component change dates)    \n","comp_rep = comp_rep.loc[comp_rep['datetime'] > pd.to_datetime('2015-01-01')]\n","\n","# replace dates of most recent component change with days since most recent component change\n","for comp in components:\n","    comp_rep[comp] = (comp_rep['datetime'] - comp_rep[comp]) / np.timedelta64(1, 'D')\n","    \n","comp_rep"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809739978}}},{"cell_type":"markdown","source":["## Machine Features\n","The machine features can be used without further modification. These include descriptive information about the type of each machine and its age (number of years in service). If the age information had been recorded as a \"first use date\" for each machine, a transformation would have been necessary to turn those into a numeric values indicating the years in service.\n","\n","Lastly, we merge all the feature data sets we created earlier to get the final feature matrix."],"metadata":{}},{"cell_type":"code","source":["telemetry_feat"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809754577}}},{"cell_type":"code","source":["final_feat = telemetry_feat.merge(error_count, on=['datetime', 'machineID'], how='left')\n","final_feat = final_feat.merge(comp_rep, on=['datetime', 'machineID'], how='left')\n","final_feat = final_feat.merge(machines, on=['machineID'], how='left')\n","\n","# merge between telemetry, number of days since maintenance and machines\n","# telemetry is the merge from 3h and 24\n","final_feat"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598809761622},"tags":[]}},{"cell_type":"markdown","source":["# Label Construction\n","When using multi-class classification for predicting failure due to a problem, labelling is done by taking a time window prior to the failure of an asset and labelling the feature records that fall into that window as \"about to fail due to a problem\" while labelling all other records as \"normal.\" This time window should be picked according to the business case: in some situations it may be enough to predict failures hours in advance, while in others days or weeks may be needed to allow e.g. for arrival of replacement parts.\n","\n","The prediction problem for this example scenerio is to estimate the probability that a machine will fail in the near future due to a failure of a certain component. More specifically, the goal is to compute the probability that a machine will fail in the next 24 hours due to a certain component failure (component 1, 2, 3, or 4). Below, a categorical failure feature is created to serve as the label. All records within a 24 hour window before a failure of component 1 have failure=comp1, and so on for components 2, 3, and 4; all records not within 24 hours of a component failure have failure=none."],"metadata":{}},{"cell_type":"code","source":["# final_feat is merge between telemetry, number of days since maintenance and machines\n","# telemetry is the merge from 3h and 24\n","# labeled_features merges also the failures\n","labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left')\n","labeled_features"],"outputs":[],"execution_count":null,"metadata":{"scrolled":true,"gather":{"logged":1598810249849}}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labeled_features = labeled_features.bfill(axis=1, limit=7) # fill backward up to 24h\n","labeled_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labeled_features = labeled_features.fillna('none')\n","labeled_features"]},{"cell_type":"markdown","source":["Below is an example of records that are labeled as failure=comp4 in the failure column. Notice that the first 8 records all occur in the 24-hour window before the first recorded failure of component 4. The next 8 records are within the 24 hour window before another failure of component 4."],"metadata":{}},{"cell_type":"code","source":["labeled_features.loc[labeled_features['failure'] == 'comp4'][:16]\n","labeled_features"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598810574888}}},{"cell_type":"markdown","source":["# Modelling\n","After the feature engineering and labelling steps, either Azure Machine Learning Studio or this notebook can be used to create a predictive model. The recommend Azure Machine Learning Studio experiment can be found in the Cortana Intelligence Gallery: Predictive Maintenance Modelling Guide Experiment. Below, we describe the modelling process and provide an example Python model.\n","\n","# Training, Validation and Testing\n","When working with time-stamped data as in this example, record partitioning into training, validation, and test sets should be performed carefully to prevent overestimating the performance of the models. In predictive maintenance, the features are usually generated using lagging aggregates: records in the same time window will likely have identical labels and similar feature values. These correlations can give a model an \"unfair advantage\" when predicting on a test set record that shares its time window with a training set record. We therefore partition records into training, validation, and test sets in large chunks, to minimize the number of time intervals shared between them.\n","\n","Predictive models have no advance knowledge of future chronological trends: in practice, such trends are likely to exist and to adversely impact the model's performance. To obtain an accurate assessment of a predictive model's performance, we recommend training on older records and validating/testing using newer records.\n","\n","For both of these reasons, a time-dependent record splitting strategy is an excellent choice for predictive maintenace models. The split is effected by choosing a point in time based on the desired size of the training and test sets: all records before the timepoint are used for training the model, and all remaining records are used for testing. (If desired, the timeline could be further divided to create validation sets for parameter selection.) To prevent any records in the training set from sharing time windows with the records in the test set, we remove any records at the boundary -- in this case, by ignoring 24 hours' worth of data prior to the timepoint."],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labeled_features"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["    from sklearn.ensemble import GradientBoostingClassifier\n","\n","    # make test and training splits\n","    threshold_dates = [[pd.to_datetime('2015-07-31 01:00:00'), pd.to_datetime('2015-08-01 01:00:00')],\n","                    [pd.to_datetime('2015-08-31 01:00:00'), pd.to_datetime('2015-09-01 01:00:00')],\n","                    [pd.to_datetime('2015-09-30 01:00:00'), pd.to_datetime('2015-10-01 01:00:00')]]\n","\n","    test_results = []\n","    models = []\n","    for last_train_date, first_test_date in threshold_dates:\n","        # split out training and test data\n","        train_y = labeled_features.loc[labeled_features['datetime'] < last_train_date, 'failure']\n","        train_X = pd.get_dummies(labeled_features.loc[labeled_features['datetime'] < last_train_date].drop(['datetime',\n","                                                                                                            'machineID',\n","                                                                                                            'failure'], 1))\n","        test_X = pd.get_dummies(labeled_features.loc[labeled_features['datetime'] > first_test_date].drop(['datetime',\n","                                                                                                        'machineID',\n","                                                                                                        'failure'], 1))                                                                                 \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"Python 3.8.5 64-bit","display_name":"Python 3.8.5 64-bit","metadata":{"interpreter":{"hash":"bbc1886ecb654281effdac017bd5adf6a19d200d2d30f9ded35574b2168481c7"}}},"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":1}